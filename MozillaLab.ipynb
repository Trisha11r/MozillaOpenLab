{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MozillaLab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trisha11r/MozillaOpenLab/blob/master/MozillaLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akmp8Du_J4Hl",
        "colab_type": "code",
        "outputId": "1cc9c93d-a8bb-44c7-d334-944846074207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Drive Mount code\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkBjYls5J6TR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd gdrive/My Drive/Mozilla\n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR0YjCKjJ1TZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Import Packages\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize \n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from joblib import dump, load\n",
        "\n",
        "import sklearn\n",
        "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipBdVbe88tN9",
        "colab_type": "text"
      },
      "source": [
        "for reference:\n",
        "\n",
        "Labels:\n",
        "0=ND, 1=D\n",
        "\n",
        "Donation_Related:\n",
        "0=request, 1=offer, -1 = N/A\n",
        "\n",
        "Resource_Type:\n",
        "0=Money, 1=Clothing, 2=Food, 3=Medical, 4=Shelter, 5=Volunteer, -1 = N/A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g2o6TRa6cvn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Stop Words\n",
        "stop_words = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\",\n",
        "                     \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\",\n",
        "                     \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\",\n",
        "                     \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\",\n",
        "                     \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\",\n",
        "                     \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\",\n",
        "                     \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\"\n",
        "                     , \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\",\n",
        "                     \"ought\", \"our\", \"ours    ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\",\n",
        "                     \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\",\n",
        "                     \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\",\n",
        "                     \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\",\n",
        "                     \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\",\n",
        "                     \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\",\n",
        "                     \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "                     \"your\", \"yours\", \"yourself\", \"yourselves\", 'a', 'about', 'above', 'across', 'after', 'again',\n",
        "                     'against', 'all', 'almost', 'alone', 'btw', 'north', 'south', 'east', 'west', 'sarita', 'woke', 'wake',\n",
        "                     'suv', 'omg', 'asap', 'contain', 'au', 'demi', 'mam', 'sir', \"ma'am\", \"i'm'\", 'ohh', 'oh', 'duh',\n",
        "                     'go', 'goes', 'went', 'gone', 'dollar', 'dollars', 'cents', 'cent', 'usa', 'dont', 'aaa',\n",
        "                     'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any',\n",
        "                     'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask',\n",
        "                     'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be',\n",
        "                     'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being',\n",
        "                     'beings', 'between', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'couldnt',\n",
        "                     'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'coz', 'd', 'did',\n",
        "                     'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'downed', 'downing', 'downs',\n",
        "                     'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even',\n",
        "                     'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f',\n",
        "                     'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'first', 'for', 'four', 'from',\n",
        "                     'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general',\n",
        "                     'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'goods', 'got',\n",
        "                     'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has',\n",
        "                     'have', 'having', 'he', 'her', 'here', 'herself',\n",
        "                     'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'into', 'is', 'it', 'its',\n",
        "                     'itself', 'j', 'just', 'k', 'keep', 'keeps',\n",
        "                     'knew', 'know', 'known', 'knows', 'l', 'largely', 'later', 'latest',\n",
        "                     'least', 'let', 'lets', 'likely', 'm', 'made', 'make',\n",
        "                     'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most',\n",
        "                     'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed',\n",
        "                     'needing', 'needs', 'new', 'next',\n",
        "                     'noone', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often',\n",
        "                     'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or',\n",
        "                     'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part',\n",
        "                     'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing',\n",
        "                     'points', 'possible', 'present', 'presented', 'presenting', 'presents',\n",
        "                     'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'room', 'rooms', 's',\n",
        "                     'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming',\n",
        "                     'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side',\n",
        "                     'sides', 'since', 'so', 'some', 'somebody', 'someone', 'something',\n",
        "                     'somewhere', 'state', 'states', 'still', 'such', 'sure', 't', 'take', 'taken', 'than',\n",
        "                     'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things',\n",
        "                     'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus',\n",
        "                     'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two',\n",
        "                     'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want',\n",
        "                     'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'We', 'well', 'wells', 'went', 'were', 'what',\n",
        "                     'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with',\n",
        "                     'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years',\n",
        "                     'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z', 'weren', 'didn', 'ours', 'hasn', 'hadn', \"should've\", 'ourselves', 're', 'wouldn', 've', 'ain', 'couldn', 'mustn', 'aren', 'isn', 'wasn', 'doesn', 'll', \"that'll\", 'mightn', 'won', 'shan', \"mightn't\", \"needn't\", 'haven', 'needn', 'ma', 'don', 'shouldn']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xECpqGmqJvvf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Clean Data\n",
        "\n",
        "# -Removing non–ASCII characters.\n",
        "# -Separating text into tokens (words), removing stopwords and performing stemming (reducing to root words, such as ‘helping’ to ‘help’).\n",
        "# -Generalizing some tokens by replacing numbers by the token _NUM_, hyperlinks by the token _URL_, retweets (“RT @user_name”) by the token _RT_ and lastly, user mentions in the tweets (@user_name) by the token _MENTION_.\n",
        "# -I/P = data to be cleaned\n",
        "# -O/P = return cleaned data\n",
        "\n",
        "# flag = 0 -> For cleaning tweets for training\n",
        "# flag = 1 -> For cleaning tweets to show on website\n",
        "def clean(data, flag):\n",
        "\t\n",
        "\tps = PorterStemmer()\n",
        "\tlem = WordNetLemmatizer()\n",
        "\n",
        "\tdata = data.replace(np.nan, 'Unknown', regex=True)\n",
        "\n",
        "\tif flag==1:\n",
        "\t\tdata['Tweet_cleaned'] = \"\"\n",
        "\telse:\n",
        "\t\tdata['Tweet'] = \"\"\n",
        "\t\n",
        "\n",
        "\tfor index, row in data.iterrows():\n",
        "\t\ts = ''\n",
        "\t\trow_tweet = row['Actual_Tweet'].replace('\\\\n', ' ')\n",
        "\t\ttweet = row_tweet.split(' ')\n",
        "\n",
        "\t\turlFound = False\n",
        "\n",
        "\t\tfor i, c in enumerate(tweet):\n",
        "\t\t\tif( c.startswith('b') and c.endswith('RT') ):\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif( c.startswith('@') and c.endswith(':') ):\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif ( 'http' in c ):\n",
        "\t\t\t\turlFound = True\n",
        "\t\t\t\t\n",
        "\t\t\t\tt = \"\"\n",
        "\t\t\t\tfor x in c:\n",
        "\t\t\t\t\tif ( (x>='a' and x<='z') or (x>='A' and x<='Z') or (x>='0' and x<='9') or x==':' or x=='/' or x==\".\"):\n",
        "\t\t\t\t\t\tt += x\n",
        "\t\t\t\t\telif x=='\\\\':\n",
        "\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tdata.at[index, 'URL'] = t\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif ( re.search(\"^.*x[a-z][0-9].*$\" , c) ):\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif ( c.startswith('#') or c.startswith('@')):\n",
        "\t\t\t\tif flag==1 and c.startswith('#'):\n",
        "\t\t\t\t\ts += c + ' '\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tif( i==0 and (c.startswith('b\\'') or c.startswith('b\\\"') ) ):\n",
        "\t\t\t\t\tc = c[2:]\n",
        "\n",
        "\t\t\t\tif flag==0:\n",
        "\t\t\t\t\tc = c.lower()\n",
        "\n",
        "\t\t\t\t\t# Removing Stop Words using nltk\n",
        "\t\t\t\t\tif c in stop_words:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\n",
        "\t\t\t\tt = ''\n",
        "\t\t\t\tind = -1\n",
        "\t\t\t\tcount = -1\n",
        "\n",
        "\t\t\t\tfor x in c:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tind += 1\n",
        "\t\t\t\t\tif flag==0 and x=='#':\n",
        "\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\t\tif ( (x>='a' and x<='z') or (x>='A' and x<='Z') ):\n",
        "\t\t\t\t\t\tt += x\n",
        "\t\t\t\t\t\tcount = 0\n",
        "\t\t\t\t\telif count==0:\n",
        "\t\t\t\t\t\tt += ' '\n",
        "\t\t\t\t\t\tcount = -1\n",
        "\n",
        "\t\t\t\tif flag==0:\n",
        "\t\t\t\t\tt = lem.lemmatize(t, pos='v')\n",
        "\t\t\t\t\tif t in stop_words:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\ts += t + ' '\n",
        "\n",
        "\t\t\n",
        "\t\t# if s=='':\n",
        "\t\t# \tprint (index)\n",
        "\n",
        "\t\tif flag==0:\n",
        "\t\t\t# Removing one, two letter words\n",
        "\t\t\ts = re.sub(r'\\b\\w{1,2}\\b', '', s)\n",
        "\n",
        "\t\t# Remove extra whitespaces in between\n",
        "\t\ts = \" \".join(s.split())\n",
        "\t\t\n",
        "\t\tif flag==0:\n",
        "\t\t\trow['Tweet'] = s\n",
        "\t\telse:\n",
        "\t\t\tdata.at[index, 'Tweet_cleaned'] = s\n",
        "\t\t\tif urlFound==False:\n",
        "\t\t\t\tdata.at[index, 'URL'] = 'Not Available'\n",
        "\t\n",
        "\tif flag==0:\t\n",
        "\t\tdata = data.drop_duplicates(subset=['Tweet'], keep='first')\n",
        "\t\tdata['Tweet'].replace('', np.nan, inplace=True)\n",
        "\t\tdata.dropna(subset=['Tweet'], inplace=True)\n",
        "\t\n",
        "\tprint ('Data cleaned')\n",
        "\n",
        "\treturn data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJjvjPXgnv9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('wordnet')\n",
        "# Clean tweets for training purpose [flag=0] - Performing Stemming, removing one or two letter words\n",
        "cleanData = clean(data, 0)\n",
        "# Clean tweets for Display purpose [flag=1] - Avoiding Stemming & removing one or two letter words\n",
        "data = clean(cleanData, 1)\n",
        "data.to_csv('final_dataset_cleaned.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB69dp781ZWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('final_dataset_cleaned.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD2ZMXkZ0RyZ",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Load Models\n",
        "loaded_models = {}\n",
        "classes = ['Labels', 'Donation_Related', 'Resource_Type']\n",
        "\n",
        "for cls in classes:\n",
        "  \n",
        "  if cls=='Labels':\n",
        "    print ('Loading Model for Donation/Non-Donation...')\n",
        "\n",
        "    loaded_models['d_nd'] = load(\"models/donation_nonDonation.joblib\")\n",
        "\n",
        "  elif cls=='Donation_Related':\n",
        "    print ('Loading Model for Only Donation Data- Request/Offer...')\n",
        "\n",
        "    loaded_models['req_off'] = load(\"models/request_offer.joblib\")\n",
        "  else:\n",
        "    print ('Loading Model for Only Donation Data- Resource Type...')\n",
        "\n",
        "    loaded_models['res_type'] = load(\"models/resource_type.joblib\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An6XJxXYx_MG",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Prediction\n",
        "\n",
        "data['D_ND'] = loaded_models['d_nd'].predict(data['Tweet'])\n",
        "data['Request_Offer'] = loaded_models['req_off'].predict(data['Tweet'])\n",
        "data['Resource_Type'] = loaded_models['res_type'].predict(data['Tweet'])\n",
        "\n",
        "data.loc[data['D_ND']==0, ['Request_Offer', 'Resource_Type']] = -1\n",
        "\n",
        "data.to_csv('final_dataset_24k.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCbkxcnz4tMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Old Dataset - 3k tweets\n",
        "data1 = pd.read_csv('final_dataset_3k.csv')\n",
        "# Old Dataset - 24k tweets\n",
        "data2 = pd.read_csv('final_dataset_24k.csv')\n",
        "\n",
        "data = pd.concat([data1, data2])\n",
        "data.to_csv('final_dataset.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVEb2bI_Oy39",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create uni, bi, tri, quad and penta tokens from sentences\n",
        "\n",
        "def create_ngramtokens(df):\n",
        "  \n",
        "  tweets = df['Tweet']\n",
        "\n",
        "  # Donation/Non-Donation\n",
        "  d_nd = df['D_ND']\n",
        "\n",
        "  # Request/Offer\n",
        "  ro = df['Request_Offer']\n",
        "\n",
        "  # Resource Type\n",
        "  typ = df['Resource_Type']\n",
        "  \n",
        "\n",
        "  features = []\n",
        "  don_nod = []\n",
        "  req_off = []\n",
        "  res_type = []\n",
        "\n",
        "  print (len(ro))\n",
        "  # i = 0\n",
        "  for ind, tweet in enumerate(tweets):\n",
        "    tweetToken = nltk.word_tokenize(tweet)\n",
        "\n",
        "    for len_grams in range(1, 5):\n",
        "\n",
        "      ngram = ngrams(tweetToken, len_grams)\n",
        "      for gram in ngram:\n",
        "        features.append([g for g in gram])\n",
        "        # print (ind)\n",
        "        don_nod.append(int(float(d_nd[ind])))\n",
        "        req_off.append(int(float(ro[ind])))\n",
        "        res_type.append(int(float(typ[ind])))\n",
        "    \n",
        "    # i += 1\n",
        " \n",
        "  data = pd.DataFrame(columns=['Features', 'D_ND', 'Request_Offer', 'Resource_Type'])\n",
        "\n",
        "  data['Features'] = features\n",
        "  data['D_ND'] = don_nod\n",
        "  data['Request_Offer'] = req_off\n",
        "  data['Resource_Type'] = res_type\n",
        "  \n",
        "\n",
        "  data.to_csv('training_data.csv', index=False)\n",
        "\n",
        "  data[data['D_ND']==1].to_csv('training_data_donation.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17TFcyImtgxa",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Training Model using Multinomial Naive Bayes\n",
        "def train():\n",
        "  classes = ['D_ND', 'Request_Offer', 'Resource_Type']\n",
        "\n",
        "  for cls in classes:\n",
        "\n",
        "    \n",
        "    if cls=='D_ND':\n",
        "      data = pd.read_csv('training_data.csv')\n",
        "      print ('Training Model for Donation/Non-Donation')\n",
        "    else:\n",
        "      data = pd.read_csv('training_data_donation.csv')\n",
        "      if cls=='Request_Offer':\n",
        "        print ('Training Model for Only Donation Data- Request/Offer')\n",
        "      else:\n",
        "        print ('Training Model for Only Donation Data- Resource Type')\n",
        "\n",
        "    # Split into training, testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data['Features'], data[cls], random_state = 0, test_size = 0.25)\n",
        "\n",
        "    \n",
        "    # define the stages of the pipeline\n",
        "    pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True)),\n",
        "                                ('model', MultinomialNB())])\n",
        "\n",
        "    # fit the pipeline model with the training data                            \n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # print (pipeline.predict(['Want to Donate PPE.']))\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    \n",
        "    if cls=='D_ND':\n",
        "      print ('Accuracy (Donation/Non-Donation): ', accuracy_score(y_test, y_pred))\n",
        "      # dump the pipeline model\n",
        "      dump(pipeline, filename=\"models/donation_nonDonation.joblib\")\n",
        "      \n",
        "    elif cls=='Request_Offer':\n",
        "      print ('Accuracy (Request/Offer): ', accuracy_score(y_test, y_pred))\n",
        "      dump(pipeline, filename=\"models/request_offer.joblib\")\n",
        "      \n",
        "    else:\n",
        "      print ('Accuracy (Resource Type): ', accuracy_score(y_test, y_pred))\n",
        "      dump(pipeline, filename=\"models/resource_type.joblib\")\n",
        "      \n",
        "    \n",
        "    print ()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NnJF9ZJ9FSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, train the model using the whole dataset\n",
        "\n",
        "data = pd.read_csv('final_dataset.csv')\n",
        "nltk.download('punkt')\n",
        "create_ngramtokens(data)\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}